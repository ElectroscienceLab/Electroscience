{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Logo_SYGNET.png\" width=\"90\" alt=\"cognitiveclass.ai logo\">\n",
    "</left>\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2d/Tensorflow_logo.svg\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A.I. Neural Training\n",
    "## Part II - Polynomial Chaos Expansion\n",
    "\n",
    "Postprocessing the simulation results from ZSOIL for Neural Network training.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By conducting this thorough data analysis, we'll gain a deeper understanding of the dataset and the underlying physical processes. This will not only help in building a better NN model but also provide valuable insights into the soil subsidence phenomenon in our simulation. These insights can guide feature selection, inform model architecture decisions, and improve interpretability of the NN's results.\n",
    "\n",
    "*   Data Science with Python\n",
    "*   Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Table of Contents</h3>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li><a href=\"#III. Feature Engineering\"><b>II. Feature Engineering</b></a></li>\n",
    "        <li><a href=\"#- Derived and Cumulative Variables\">- Correlation and Causality</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.9.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.8.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.21.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (23.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.34.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
      "Requirement already satisfied: numpy==1.21.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.9.0\n",
    "!pip install numpy==1.21.4\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.integrate import dblquad\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from statsmodels.stats.stattools import omni_normtest, jarque_bera\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the <i>AI_Dataset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIME', 'SF', 'PUSHOVER LABEL', 'PUSHOVER LAMBDA', 'PUSHOVER U-CTRL', 'ARC LENGTH STEP', 'ARC LENGTH U-NORM', 'ARC LENGTH LOAD FACTOR', 'NR', 'X', 'Y', 'Displacement-X', 'Displacement-Y', 'Rotation-Z', 'Total head-', 'Residual Force-X', 'Residual Force-Y', 'Residual Heat flux-Z', 'Solid-Velocity-X', 'Solid-Velocity-Y', 'Solid-Acceleration-X', 'Solid-Acceleration-Y', 'Unnamed: 23', 'TIME.1', 'SF.1', 'PUSHOVER LABEL.1', 'PUSHOVER LAMBDA.1', 'PUSHOVER U-CTRL.1', 'ARC LENGTH STEP.1', 'ARC LENGTH U-NORM.1', 'ARC LENGTH LOAD FACTOR.1', 'ELEM.', 'GP', 'Z', 'Eff.Stress-XY', 'Eff.Stress-XZ', 'Eff.Stress-YZ', 'Tot.Stress-XZ', 'Tot.Stress-YZ', 'Strain-XX', 'Strain-YY', 'Strain-XY', 'Strain-ZZ', 'Strain-XZ', 'Strain-YZ', 'Strain-11', 'Strain-33', 'Strain-J2^1/2', 'Stress level', 'Saturation', 'Fluid velocity-X', 'Fluid velocity-Y', 'Fluid velocity-ABS', 'Pc', 'Pore pressure', 'alpha*(S*pF+<dpF_undr>)', 'Temperature', 'Unnamed: 60', 'Gradient_DisplacementY_X', 'Gradient_StressYY_X', 'Gradient_StressYY_Y', 'Normalized_DisplacementY', 'Normalized_StressYY', 'Displacement-Magnitude', 'Cumulative-Displacement-Y', 'StressYY_DisplacementY_Interaction', 'DisplacementY_Rate', 'StressYY_Rate', 'Cumulative_StressYY', 'Log_StressYY', 'PCA_1', 'PCA_2', 'PCA_3', 'Stress_Regime', 'PIM_Subsidence']\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "subsidence_data = pd.read_csv('PIM_Subsidence.csv')\n",
    "print(subsidence_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Metamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "subsidence_scaler = StandardScaler()\n",
    "data['Subsidence'] = subsidence_scaler.fit_transform(data[['Subsidence']])\n",
    "\n",
    "# Define a mapping from string to numeric values\n",
    "mapping = {'elastic': 0, 'plastic': 1, 'brittle': 2}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "data['Stress_Regime'] = data['Stress_Regime'].map(mapping)\n",
    "\n",
    "# Handle any missing or unmapped values\n",
    "data['Stress_Regime'] = data['Stress_Regime'].fillna(-1)  # or any other default value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Chebyshev Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Chebyshev polynomials of the first kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Define Chebyshev polynomial of degree 3\n",
    "T3 = chebyt(3)\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = T3(x)\n",
    "\n",
    "# Plotting the polynomial\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, label='T3(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Jacobi Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the parameters (\\alpha) and (\\beta), which define the type of Jacobi polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import jacobi\n",
    "\n",
    "# Example: Define Jacobi polynomial of degree 3 with alpha=0.5 and beta=0.5\n",
    "J3 = jacobi(3, 0.5, 0.5)\n",
    "y_jacobi = J3(x)\n",
    "\n",
    "# Plotting the polynomial\n",
    "plt.plot(x, y_jacobi, label='J3(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the PCE Metamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the PCE metamodel, you need to create a basis of polynomials and then use them to approximate your model. Here’s a simplified example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.uniform(-1, 1, (100, 1))\n",
    "y = np.sin(X).ravel() + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Define polynomial basis functions\n",
    "def chebyshev_basis(x, degree):\n",
    "    return np.array([chebyt(d)(x) for d in range(degree + 1)]).T\n",
    "\n",
    "def jacobi_basis(x, degree, alpha, beta):\n",
    "    return np.array([jacobi(d, alpha, beta)(x) for d in range(degree + 1)]).T\n",
    "\n",
    "# Combine basis functions\n",
    "degree = 3\n",
    "alpha, beta = 0.5, 0.5\n",
    "X_chebyshev = chebyshev_basis(X, degree)\n",
    "X_jacobi = jacobi_basis(X, degree, alpha, beta)\n",
    "X_combined = np.hstack((X_chebyshev, X_jacobi))\n",
    "\n",
    "# Fit the PCE model\n",
    "model = LinearRegression()\n",
    "model.fit(X_combined, y)\n",
    "\n",
    "# Predict using the PCE model\n",
    "y_pred = model.predict(X_combined)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, y_pred, label='PCE Model', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the degree of the polynomials or the parameters (\\alpha) and (\\beta) for Jacobi polynomials to better capture the characteristics of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[SciPy Documentation on Chebyshev Polynomials](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.chebyt.html) \n",
    "\n",
    "[SciPy Documentation on Jacobi Polynomials](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.chebyt.html) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chaospy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (4.3.16)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from chaospy) (1.21.4)\n",
      "Requirement already satisfied: numpoly>=1.2.12 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from chaospy) (1.2.13)\n",
      "Requirement already satisfied: scipy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from chaospy) (1.7.3)\n",
      "Requirement already satisfied: setuptools>=40.9.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from chaospy) (67.7.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from chaospy) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->chaospy) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->chaospy) (4.5.0)\n",
      "Requirement already satisfied: openturns in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.19.post1)\n",
      "Requirement already satisfied: dill in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from openturns) (0.3.7)\n",
      "Requirement already satisfied: psutil in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from openturns) (5.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install chaospy\n",
    "!pip install openturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chaospy as cp\n",
    "import openturns as ot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openTURNS Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally map PIM_Subsidence by StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "subsidence_scaler = StandardScaler()\n",
    "subsidence_data['PIM_Subsidence'] = subsidence_scaler.fit_transform(subsidence_data[['PIM_Subsidence']])\n",
    "\n",
    "# Define a mapping from string to numeric values\n",
    "mapping = {'elastic': 0, 'plastic': 1, 'brittle': 2}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "subsidence_data['Stress_Regime'] = subsidence_data['Stress_Regime'].map(mapping)\n",
    "\n",
    "# Handle any missing or unmapped values\n",
    "subsidence_data['Stress_Regime'] = subsidence_data['Stress_Regime'].fillna(-1)  # or any other default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tailoring the 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded lstm_train_predictions: (15973,)\n",
      "Shape of padded lstm_test_predictions: (15973,)\n",
      "Shape of lstm_train_predictions: (15973,)\n",
      "Shape of lstm_test_predictions: (15973,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "lstm_train_predictions_df = pd.read_csv('train_predictions.csv')\n",
    "lstm_test_predictions_df = pd.read_csv('test_predictions.csv')\n",
    "subsidence_data = pd.read_csv('PIM_Subsidence.csv')\n",
    "\n",
    "# Convert 'Stress Regime' categorical column to numerical using one-hot encoding\n",
    "subsidence_data = pd.get_dummies(subsidence_data, columns=['Stress_Regime'])\n",
    "\n",
    "# Convert predictions to numeric arrays\n",
    "lstm_train_predictions = lstm_train_predictions_df.values.flatten().astype(float)\n",
    "lstm_test_predictions = lstm_test_predictions_df.values.flatten().astype(float)\n",
    "\n",
    "# Calculate the difference in length\n",
    "difference = len(subsidence_data) - len(lstm_train_predictions)\n",
    "\n",
    "# Pad the smaller dataset by repeating the last value\n",
    "lstm_train_predictions_padded = np.pad(lstm_train_predictions, (0, difference), 'edge')\n",
    "print(\"Shape of padded lstm_train_predictions:\", lstm_train_predictions_padded.shape)\n",
    "\n",
    "# Calculate the difference in length\n",
    "difference = len(lstm_train_predictions_padded) - len(lstm_test_predictions)\n",
    "\n",
    "# Pad the smaller dataset by repeating the last value\n",
    "lstm_test_predictions_padded = np.pad(lstm_test_predictions, (0, difference), 'edge')\n",
    "lstm_test_predictions=lstm_test_predictions_padded\n",
    "lstm_train_predictions=lstm_train_predictions_padded\n",
    "# Check the new shape\n",
    "print(\"Shape of padded lstm_test_predictions:\", lstm_test_predictions_padded.shape)\n",
    "print(f\"Shape of lstm_train_predictions: {lstm_train_predictions.shape}\")\n",
    "print(f\"Shape of lstm_test_predictions: {lstm_train_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (12778, 74)\n",
      "Shape of X_test: (3195, 74)\n",
      "Shape of y_train: (12778,)\n",
      "Shape of y_test: (3195,)\n",
      "Shape of X_train: (12778, 74)\n",
      "Shape of X_test: (3195, 74)\n",
      "Shape of y_train: (12778,)\n",
      "Shape of y_test: (3195,)\n",
      "Shape of y_train: (12778,)\n",
      "Shape of lstm_train_predictions: (15973,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'PIM_Subsidence' is the target column\n",
    "X = subsidence_data.drop(columns=['PIM_Subsidence']).values  # Features\n",
    "y = subsidence_data['PIM_Subsidence'].values  # Target\n",
    "\n",
    "# Calculate test_size as a proportion of the total data length\n",
    "test_size = len(lstm_test_predictions) / len(subsidence_data)\n",
    "\n",
    "# Determine the split index based on the length of the training predictions\n",
    "split_index = len(lstm_train_predictions)\n",
    "\n",
    "# Manually split the data\n",
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Ensure the test size is reasonable (e.g., 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Similarly split the LSTM predictions to match the train/test split\n",
    "lstm_train_predictions_split, lstm_test_predictions_split = train_test_split(\n",
    "    lstm_train_predictions, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate residuals for training set\n",
    "train_residuals = y_train - lstm_train_predictions_split.flatten()\n",
    "\n",
    "# Calculate residuals for test set\n",
    "test_residuals = y_test - lstm_test_predictions_split.flatten()\n",
    "\n",
    "\n",
    "# Ensure that the shapes are correct\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Ensure the length of predictions matches the respective training and test data\n",
    "#assert len(lstm_train_predictions) == len(y_train), \"Mismatch in the length of training predictions\"\n",
    "#assert len(lstm_test_predictions) == len(y_test), \"Mismatch in the length of test predictions\"\n",
    "\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of lstm_train_predictions: {lstm_train_predictions.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure that X_train is a numeric array\n",
    "X_train = X_train.astype(float)\n",
    "\n",
    "\n",
    "# Define the distributions for each input feature\n",
    "#distributions = [cp.Uniform(np.min(X_train[:, i]), np.max(X_train[:, i])) for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create the joint distribution for all input features\n",
    "#joint_distribution = cp.J(*distributions)\n",
    "\n",
    "\n",
    "# Define a small epsilon value\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Define the distributions for each input feature\n",
    "distributions = [ot.Uniform(np.min(X_train[:, i]), np.max(X_train[:, i]) + epsilon) for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create the joint distribution for all input features\n",
    "joint_distribution = ot.ComposedDistribution(distributions)\n",
    "\n",
    "# Generate a sample from the joint distribution\n",
    "sample = joint_distribution.getSample(X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Polynomial Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define Chebyshev polynomial basis of order 3\n",
    "chebyshev_basis = ot.OrthogonalUniVariatePolynomialFamily(ot.ChebychevFactory())\n",
    "\n",
    "# Define Jacobi polynomial basis of order 3 with alpha=0 and beta=0\n",
    "jacobi_basis = ot.OrthogonalUniVariatePolynomialFamily(ot.JacobiFactory(0, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Chaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'new_FixedStrategy'.\n  Possible C/C++ prototypes are:\n    OT::FixedStrategy::FixedStrategy()\n    OT::FixedStrategy::FixedStrategy(OT::OrthogonalBasis const &,OT::UnsignedInteger const)\n    OT::FixedStrategy::FixedStrategy(OT::FixedStrategy const &)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1609/27268514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbasis_chebyshev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrthogonalProductPolynomialFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChebychevFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0madaptive_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFixedStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprojection_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeastSquaresStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/openturns/metamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         \u001b[0m_metamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFixedStrategy_swiginit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_metamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_FixedStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_metamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_FixedStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'new_FixedStrategy'.\n  Possible C/C++ prototypes are:\n    OT::FixedStrategy::FixedStrategy()\n    OT::FixedStrategy::FixedStrategy(OT::OrthogonalBasis const &,OT::UnsignedInteger const)\n    OT::FixedStrategy::FixedStrategy(OT::FixedStrategy const &)\n"
     ]
    }
   ],
   "source": [
    "sample = joint_distribution.getSample(X_train.shape[0])\n",
    "\n",
    "basis_chebyshev = ot.OrthogonalProductPolynomialFactory([ot.ChebychevFactory()] * X_train.shape[1])\n",
    "adaptive_strategy = ot.FixedStrategy(sample)\n",
    "projection_strategy = ot.LeastSquaresStrategy()\n",
    "\n",
    "pce_chebyshev = ot.FunctionalChaosAlgorithm(sample, train_residuals, joint_distribution, adaptive_strategy, projection_strategy)\n",
    "pce_chebyshev.run()\n",
    "result_chebyshev = pce_chebyshev.getResult()\n",
    "metamodel_chebyshev = result_chebyshev.getMetaModel()\n",
    "\n",
    "basis_jacobi = ot.OrthogonalProductPolynomialFactory([ot.JacobiFactory()] * X_train.shape[1])\n",
    "pce_jacobi = ot.FunctionalChaosAlgorithm(sample, train_residuals, joint_distribution, adaptive_strategy, projection_strategy)\n",
    "pce_jacobi.run()\n",
    "result_jacobi = pce_jacobi.getResult()\n",
    "metamodel_jacobi = result_jacobi.getMetaModel()\n",
    "residual_pred_chebyshev = np.array([metamodel_chebyshev(X_test[i, :]) for i in range(X_test.shape[0])])\n",
    "residual_pred_jacobi = np.array([metamodel_jacobi(X_test[i, :]) for i in range(X_test.shape[0])])\n",
    "\n",
    "final_pred_chebyshev = lstm_test_predictions.flatten() + residual_pred_chebyshev\n",
    "final_pred_jacobi = lstm_test_predictions.flatten() + residual_pred_jacobi\n",
    "\n",
    "mse_chebyshev = mean_squared_error(y_test, final_pred_chebyshev)\n",
    "mse_jacobi = mean_squared_error(y_test, final_pred_jacobi)\n",
    "\n",
    "print(f\"Mean Squared Error with Chebyshev basis: {mse_chebyshev}\")\n",
    "print(f\"Mean Squared Error with Jacobi basis: {mse_jacobi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openturns as ot\n",
    "\n",
    "# Define the Chebyshev polynomial factory for each feature\n",
    "chebyshev_basis_factory = ot.ChebychevFactory()\n",
    "\n",
    "# Define the Jacobi polynomial factory for each feature\n",
    "jacobi_basis_factory = ot.JacobiFactory(0, 0)\n",
    "\n",
    "# Combine them into a multivariate polynomial basis\n",
    "chebyshev_collection = [chebyshev_basis_factory for _ in range(X_train.shape[1])]\n",
    "jacobi_collection = [jacobi_basis_factory for _ in range(X_train.shape[1])]\n",
    "\n",
    "# Create the orthogonal product polynomial factories\n",
    "basis_chebyshev = ot.OrthogonalProductPolynomialFactory(chebyshev_collection)\n",
    "basis_jacobi = ot.OrthogonalProductPolynomialFactory(jacobi_collection)\n",
    "# Create a linear enumerate function based on the number of features\n",
    "enumerate_function = ot.LinearEnumerateFunction(X_train.shape[1])\n",
    "\n",
    "# Set the desired polynomial degree\n",
    "total_degree = 3  # Example: total degree of polynomials\n",
    "\n",
    "# Determine the strata index that corresponds to the desired polynomial degree\n",
    "strata_index = enumerate_function.getMaximumDegreeStrataIndex(total_degree)\n",
    "\n",
    "# Calculate the cumulative cardinality up to the desired strata index\n",
    "basis_size = enumerate_function.getStrataCumulatedCardinal(strata_index)\n",
    "\n",
    "# Create the fixed strategy for Chebyshev basis\n",
    "adaptive_strategy_chebyshev = ot.FixedStrategy(basis_chebyshev, basis_size)\n",
    "\n",
    "# Create the fixed strategy for Jacobi basis\n",
    "adaptive_strategy_jacobi = ot.FixedStrategy(basis_jacobi, basis_size)\n",
    "# Projection strategy using Least Squares\n",
    "projection_strategy = ot.LeastSquaresStrategy()\n",
    "\n",
    "# Sample from the joint distribution\n",
    "sample = joint_distribution.getSample(X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1609/2923433520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assume `sample_data` and `train_residuals_data` are provided as NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_residuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_residuals_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the joint distribution (using a multivariate normal as an example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume `sample_data` and `train_residuals_data` are provided as NumPy arrays\n",
    "sample = ot.Sample(sample_data)\n",
    "train_residuals = ot.Sample(train_residuals_data)\n",
    "\n",
    "# Define the joint distribution (using a multivariate normal as an example)\n",
    "dimension = sample.getDimension()\n",
    "marginals = [ot.Normal()] * dimension\n",
    "copula = ot.IndependentCopula(dimension)\n",
    "joint_distribution = ot.ComposedDistribution(marginals, copula)\n",
    "\n",
    "# Define the orthogonal basis and adaptive strategy\n",
    "polynomials = ot.StandardDistributionPolynomialFactory(ot.Normal())\n",
    "basis = ot.OrthogonalProductPolynomialFactory([polynomials] * dimension)\n",
    "adaptive_strategy_chebyshev = ot.FixedStrategy(basis, 10)\n",
    "\n",
    "# Define the projection strategy using least squares\n",
    "projection_strategy = ot.LeastSquaresStrategy()\n",
    "\n",
    "# Construct and run the FunctionalChaosAlgorithm\n",
    "pce_chebyshev = ot.FunctionalChaosAlgorithm(sample, train_residuals, joint_distribution, adaptive_strategy_chebyshev, projection_strategy)\n",
    "pce_chebyshev.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAOSPy approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the PCE result and metamodel\n",
    "result_chebyshev = pce_chebyshev.getResult()\n",
    "metamodel_chebyshev = result_chebyshev.getMetaModel()\n",
    "\n",
    "# Run the PCE algorithm for Jacobi basis\n",
    "pce_jacobi = ot.FunctionalChaosAlgorithm(sample, train_residuals, joint_distribution, adaptive_strategy_jacobi, projection_strategy)\n",
    "pce_jacobi.run()\n",
    "\n",
    "result_jacobi = pce_jacobi.getResult()\n",
    "metamodel_jacobi = result_jacobi.getMetaModel()\n",
    "# Predict residuals using the Chebyshev metamodel\n",
    "residual_pred_chebyshev = np.array([metamodel_chebyshev(X_test[i, :]) for i in range(X_test.shape[0])])\n",
    "\n",
    "# Combine residual predictions with the original LSTM predictions\n",
    "final_pred_chebyshev = lstm_test_predictions.flatten() + residual_pred_chebyshev\n",
    "\n",
    "# Calculate mean squared error for Chebyshev basis\n",
    "mse_chebyshev = mean_squared_error(y_test, final_pred_chebyshev)\n",
    "print(f\"Mean Squared Error with Chebyshev basis: {mse_chebyshev}\")\n",
    "\n",
    "# Predict residuals using the Jacobi metamodel\n",
    "residual_pred_jacobi = np.array([metamodel_jacobi(X_test[i, :]) for i in range(X_test.shape[0])])\n",
    "\n",
    "# Combine residual predictions with the original LSTM predictions\n",
    "final_pred_jacobi = lstm_test_predictions.flatten() + residual_pred_jacobi\n",
    "\n",
    "# Calculate mean squared error for Jacobi basis\n",
    "mse_jacobi = mean_squared_error(y_test, final_pred_jacobi)\n",
    "print(f\"Mean Squared Error with Jacobi basis: {mse_jacobi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 12778)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Create an interpolation function\n",
    "interp_func = interp1d(np.arange(train_residuals.size), train_residuals, kind='linear')\n",
    "\n",
    "# Generate new indices for interpolation\n",
    "new_indices = np.linspace(0, train_residuals.size - 1, 74 * 12778)\n",
    "\n",
    "# Interpolate the data\n",
    "train_residuals_interpolated = interp_func(new_indices).reshape(74, 12778)\n",
    "\n",
    "print(train_residuals_interpolated.shape)  # Should be (74, 12778)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 12778)\n",
      "(12778,)\n",
      "(74, 12778)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.T.shape)\n",
    "print(train_residuals.shape)\n",
    "\n",
    "# Repeat the data to fit the new shape\n",
    "train_residuals_repeated = np.tile(train_residuals, (74, 1))\n",
    "\n",
    "print(train_residuals_repeated.shape)  # Should be (74, 12778)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1609/1785584310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit polynomial chaos expansions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpce_chebyshev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchebyshev_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_residuals_interpolated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpce_jacobi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobi_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_residuals_interpolated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Predict residuals for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/chaospy/regression.py\u001b[0m in \u001b[0;36mfit_regression\u001b[0;34m(polynomials, abscissas, evals, model, retall)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mabscissas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mpoly_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolynomials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mabscissas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit polynomial chaos expansions\n",
    "pce_chebyshev = cp.fit_regression(chebyshev_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "pce_jacobi = cp.fit_regression(jacobi_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "\n",
    "# Predict residuals for the test set\n",
    "residual_pred_chebyshev = pce_chebyshev(X_test.T)\n",
    "residual_pred_jacobi = pce_jacobi(X_test.T)\n",
    "\n",
    "# Calculate final predictions\n",
    "final_pred_chebyshev = lstm_test_predictions.flatten() + residual_pred_chebyshev\n",
    "final_pred_jacobi = lstm_test_predictions.flatten() + residual_pred_jacobi\n",
    "\n",
    "# Calculate mean squared errors\n",
    "mse_chebyshev = mean_squared_error(y_test, final_pred_chebyshev)\n",
    "mse_jacobi = mean_squared_error(y_test, final_pred_jacobi)\n",
    "\n",
    "print(f\"Mean Squared Error with Chebyshev basis: {mse_chebyshev}\")\n",
    "print(f\"Mean Squared Error with Jacobi basis: {mse_jacobi}\")\n",
    "print(X_train.T.shape)\n",
    "print(train_residuals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 12778)\n",
      "(74, 12778)\n",
      "AssertionError: \n"
     ]
    }
   ],
   "source": [
    "# Ensure the shapes are correct\n",
    "print(X_train.T.shape)  # Should be (74, 12778)\n",
    "print(train_residuals_interpolated.shape)  # Should be (74, 12778)\n",
    "\n",
    "# Fit polynomial chaos expansions\n",
    "try:\n",
    "    pce_chebyshev = cp.fit_regression(chebyshev_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "    pce_jacobi = cp.fit_regression(jacobi_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chaospy.orth_ttr name is to be deprecated; Use chaospy.expansion.stieltjes instead\n"
     ]
    }
   ],
   "source": [
    "import chaospy as cp\n",
    "import openturns as ot\n",
    "import numpy as np\n",
    "\n",
    "epsilon = 1e-6  # Small value to avoid zero range\n",
    "distributions = [ot.Uniform(np.min(X_train[:, i]), np.max(X_train[:, i]) + epsilon) for i in range(X_train.shape[1])]\n",
    "joint_distribution = ot.ComposedDistribution(distributions)\n",
    "\n",
    "cp_distributions = cp.J(*[cp.Uniform(np.min(X_train[:, i]), np.max(X_train[:, i]) + epsilon) for i in range(X_train.shape[1])])\n",
    "chebyshev_basis = cp.orth_ttr(3, cp_distributions)  # Order 3 Chebyshev basis\n",
    "jacobi_basis = cp.orth_ttr(3, cp_distributions)  # Order 3 Jacobi basis with alpha=0 and beta=0\n",
    "\n",
    "try:\n",
    "    pce_chebyshev = cp.fit_regression(chebyshev_basis, cp_distributions, X_train.T, train_residuals_interpolated)\n",
    "    pce_jacobi = cp.fit_regression(jacobi_basis, cp_distributions, X_train.T, train_residuals_interpolated)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit polynomial chaos expansions\n",
    "pce_chebyshev = cp.fit_regression(chebyshev_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "pce_jacobi = cp.fit_regression(jacobi_basis, joint_distribution, X_train.T, train_residuals_interpolated)\n",
    "\n",
    "# Predict residuals for the test set\n",
    "residual_pred_chebyshev = pce_chebyshev(X_test.T)\n",
    "residual_pred_jacobi = pce_jacobi(X_test.T)\n",
    "\n",
    "# Calculate final predictions\n",
    "final_pred_chebyshev = lstm_test_predictions.flatten() + residual_pred_chebyshev\n",
    "final_pred_jacobi = lstm_test_predictions.flatten() + residual_pred_jacobi\n",
    "\n",
    "# Calculate mean squared errors\n",
    "mse_chebyshev = mean_squared_error(y_test, final_pred_chebyshev)\n",
    "mse_jacobi = mean_squared_error(y_test, final_pred_jacobi)\n",
    "\n",
    "print(f\"Mean Squared Error with Chebyshev basis: {mse_chebyshev}\")\n",
    "print(f\"Mean Squared Error with Jacobi basis: {mse_jacobi}\")\n",
    "print(X_train.T.shape)\n",
    "print(train_residuals.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
