{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Logo_SYGNET.png\" width=\"90\" alt=\"cognitiveclass.ai logo\">\n",
    "</left>\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2d/Tensorflow_logo.svg\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A.I. Neural Training\n",
    "## Part I - Architecture and Design of LSTM Network\n",
    "\n",
    "Postprocessing the simulation results from ZSOIL for Neural Network training.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By conducting this thorough data analysis, we'll gain a deeper understanding of the dataset and the underlying physical processes. This will not only help in building a better NN model but also provide valuable insights into the soil subsidence phenomenon in our simulation. These insights can guide feature selection, inform model architecture decisions, and improve interpretability of the NN's results.\n",
    "\n",
    "*   Data Science with Python\n",
    "*   Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Table of Contents</h3>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li><a href=\"#III. Feature Engineering\"><b>II. Feature Engineering</b></a></li>\n",
    "        <li><a href=\"#- Derived and Cumulative Variables\">- Correlation and Causality</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from statsmodels) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from statsmodels) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.21 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from statsmodels) (1.3.5)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from patsy>=0.5->statsmodels) (1.16.0)\n",
      "1.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from statsmodels.stats.stattools import omni_normtest, jarque_bera\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install TensorFlow for our operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.9.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.8.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.21.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (23.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.34.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
      "Requirement already satisfied: numpy==1.21.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.9.0\n",
    "!pip install numpy==1.21.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.integrate import dblquad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the <i>AI_Dataset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIME', 'SF', 'PUSHOVER LABEL', 'PUSHOVER LAMBDA', 'PUSHOVER U-CTRL', 'ARC LENGTH STEP', 'ARC LENGTH U-NORM', 'ARC LENGTH LOAD FACTOR', 'NR', 'X', 'Y', 'Displacement-X', 'Displacement-Y', 'Rotation-Z', 'Total head-', 'Residual Force-X', 'Residual Force-Y', 'Residual Heat flux-Z', 'Solid-Velocity-X', 'Solid-Velocity-Y', 'Solid-Acceleration-X', 'Solid-Acceleration-Y', 'Unnamed: 23', 'TIME.1', 'SF.1', 'PUSHOVER LABEL.1', 'PUSHOVER LAMBDA.1', 'PUSHOVER U-CTRL.1', 'ARC LENGTH STEP.1', 'ARC LENGTH U-NORM.1', 'ARC LENGTH LOAD FACTOR.1', 'ELEM.', 'GP', 'Z', 'Eff.Stress-XY', 'Eff.Stress-XZ', 'Eff.Stress-YZ', 'Tot.Stress-XZ', 'Tot.Stress-YZ', 'Strain-XX', 'Strain-YY', 'Strain-XY', 'Strain-ZZ', 'Strain-XZ', 'Strain-YZ', 'Strain-11', 'Strain-33', 'Strain-J2^1/2', 'Stress level', 'Saturation', 'Fluid velocity-X', 'Fluid velocity-Y', 'Fluid velocity-ABS', 'Pc', 'Pore pressure', 'alpha*(S*pF+<dpF_undr>)', 'Temperature', 'Unnamed: 60', 'Gradient_DisplacementY_X', 'Gradient_StressYY_X', 'Gradient_StressYY_Y', 'Normalized_DisplacementY', 'Normalized_StressYY', 'Displacement-Magnitude', 'Cumulative-Displacement-Y', 'StressYY_DisplacementY_Interaction', 'DisplacementY_Rate', 'StressYY_Rate', 'Cumulative_StressYY', 'Log_StressYY', 'PCA_1', 'PCA_2', 'PCA_3', 'Stress_Regime']\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "training_data = pd.read_csv('AI_Dataset.csv')\n",
    "print(training_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Subsidence with probability integral method (PIM) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv('AI_Dataset.csv')\n",
    "\n",
    "# Pre-compute the cKDTree for efficient neighbor searching\n",
    "coordinates = training_data[['X', 'Y']].values\n",
    "tree = cKDTree(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simplified q_function\n",
    "def q_function(x_idx, training_data):\n",
    "    row = training_data.iloc[x_idx]\n",
    "    return (\n",
    "        row['Displacement-Y'] * 0.3 + \n",
    "        row['Eff.Stress-XY'] * 0.2 + \n",
    "        row['Fluid velocity-Y'] * 0.1 + \n",
    "        row['Saturation'] * 0.1 + \n",
    "        row['Pore pressure'] * 0.1 + \n",
    "        row['Gradient_DisplacementY_X'] * 0.05 + \n",
    "        row['Gradient_StressYY_X'] * 0.05 + \n",
    "        row['Gradient_StressYY_Y'] * 0.05 + \n",
    "        row['Normalized_DisplacementY'] * 0.05 + \n",
    "        row['Normalized_StressYY'] * 0.05 + \n",
    "        row['Displacement-Magnitude'] * 0.05 + \n",
    "        row['Cumulative-Displacement-Y'] * 0.05 + \n",
    "        row['StressYY_DisplacementY_Interaction'] * 0.05 + \n",
    "        row['DisplacementY_Rate'] * 0.05 + \n",
    "        row['StressYY_Rate'] * 0.05 + \n",
    "        row['Cumulative_StressYY'] * 0.05 + \n",
    "        row['Log_StressYY'] * 0.05 + \n",
    "        row['PCA_1'] * 0.05 + \n",
    "        row['PCA_2'] * 0.05 + \n",
    "        row['PCA_3'] * 0.05\n",
    "    )\n",
    "\n",
    "# Optimized subsidence calculation with limited neighborhood and simplified summation\n",
    "def calculate_subsidence_pim(x_idx, training_data, n=2, neighborhood_radius=5):\n",
    "    x, y = training_data.iloc[x_idx][['X', 'Y']].values\n",
    "    neighbors_idx = tree.query_ball_point([x, y], neighborhood_radius)\n",
    "    \n",
    "    subsidence = sum(\n",
    "        q_function(neighbor_idx, training_data) / \n",
    "        max(np.sqrt((x - training_data.iloc[neighbor_idx]['X'])**2 +\n",
    "                    (y - training_data.iloc[neighbor_idx]['Y'])**2), 1e-6) ** n\n",
    "        for neighbor_idx in neighbors_idx\n",
    "    )\n",
    "    \n",
    "    return subsidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Variant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply subsidence calculation to the dataset in parallel\n",
    "def apply_subsidence_pim_to_dataset(training_data, n=2, neighborhood_radius=5):\n",
    "    results = Parallel(n_jobs=-1, backend='loky', prefer=\"threads\")(\n",
    "        delayed(calculate_subsidence_pim)(idx, training_data, n, neighborhood_radius) \n",
    "        for idx in range(len(training_data))\n",
    "    )\n",
    "    training_data['PIM_Subsidence'] = results\n",
    "    return training_data\n",
    "\n",
    "# Calculate subsidence using PIM and add to the dataset\n",
    "training_data = apply_subsidence_pim_to_dataset(training_data)\n",
    "\n",
    "# Save or use the updated training_data with the PIM_Subsidence column\n",
    "training_data.to_csv('PIM_Subsidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save or use the updated training_data with the PIM_Subsidence column\n",
    "training_data.to_csv('PIM_Subsidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Variant 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply subsidence calculation to the dataset in parallel\n",
    "def apply_subsidence_pim_to_dataset(training_data, n=2, neighborhood_radius=5):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_subsidence_pim)(idx, training_data, n, neighborhood_radius) \n",
    "        for idx in range(len(training_data))\n",
    "    )\n",
    "    training_data['PIM_Subsidence'] = results\n",
    "    return training_data\n",
    "\n",
    "# Calculate subsidence using PIM and add to the dataset\n",
    "training_data = apply_subsidence_pim_to_dataset(training_data)\n",
    "\n",
    "# Save or use the updated training_data with the PIM_Subsidence column\n",
    "training_data.to_csv('PIM_Subsidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler Model for Artificial Subsidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Artificial Subsidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AI_Dataset.csv')\n",
    "\n",
    "data['Subsidence'] = (data['Displacement-Y'] * 0.3 + \n",
    "                      data['Eff.Stress-XY'] * 0.2 + \n",
    "                      data['Fluid velocity-Y'] * 0.1 + \n",
    "                      data['Saturation'] * 0.1 + \n",
    "                      data['Pore pressure'] * 0.1 + \n",
    "                      data['Gradient_DisplacementY_X'] * 0.05 + \n",
    "                      data['Gradient_StressYY_X'] * 0.05 + \n",
    "                      data['Gradient_StressYY_Y'] * 0.05 + \n",
    "                      data['Normalized_DisplacementY'] * 0.05 + \n",
    "                      data['Normalized_StressYY'] * 0.05 + \n",
    "                      data['Displacement-Magnitude'] * 0.05 + \n",
    "                      data['Cumulative-Displacement-Y'] * 0.05 + \n",
    "                      data['StressYY_DisplacementY_Interaction'] * 0.05 + \n",
    "                      data['DisplacementY_Rate'] * 0.05 + \n",
    "                      data['StressYY_Rate'] * 0.05 + \n",
    "                      data['Cumulative_StressYY'] * 0.05 + \n",
    "                      data['Log_StressYY'] * 0.05 + \n",
    "                      data['PCA_1'] * 0.05 + \n",
    "                      data['PCA_2'] * 0.05 + \n",
    "                      data['PCA_3'] * 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "subsidence_scaler = StandardScaler()\n",
    "data['Subsidence'] = subsidence_scaler.fit_transform(data[['Subsidence']])\n",
    "\n",
    "# Define a mapping from string to numeric values\n",
    "mapping = {'elastic': 0, 'plastic': 1, 'brittle': 2}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "data['Stress_Regime'] = data['Stress_Regime'].map(mapping)\n",
    "\n",
    "# Handle any missing or unmapped values\n",
    "data['Stress_Regime'] = data['Stress_Regime'].fillna(-1)  # or any other default value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for artificial Subsidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select both original and engineered features for the LSTM model\n",
    "features = data[['Displacement-Y','Eff.Stress-XY', 'Fluid velocity-Y', 'Solid-Velocity-Y', 'Saturation', 'Pore pressure',\n",
    "                 'Gradient_DisplacementY_X', 'Gradient_StressYY_X', 'Gradient_StressYY_Y', \n",
    "                 'Normalized_DisplacementY', 'Normalized_StressYY', 'Displacement-Magnitude', \n",
    "                 'Cumulative-Displacement-Y', 'StressYY_DisplacementY_Interaction', 'DisplacementY_Rate', \n",
    "                 'StressYY_Rate', 'Cumulative_StressYY', 'Log_StressYY', 'PCA_1', 'PCA_2', 'PCA_3'] + \n",
    "                 [col for col in data.columns if 'Stress_Regime' in col]]\n",
    "\n",
    "# Target variable\n",
    "target = data['Subsidence']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features (necessary for LSTM to learn effectively)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input data to 3D for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 1, X_valid.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Reshape targets to match LSTM input requirements\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_valid = y_valid.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding LSTM layers with dropout\n",
    "model.add(tf.keras.layers.LSTM(units=256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.LSTM(units=128, return_sequences=False))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Add Dense layer for output prediction\n",
    "model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_valid, y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions back to the original scale if needed (e.g., if you inverse transform the target)\n",
    "# y_pred_original = scaler.inverse_transform(y_pred)\n",
    "# y_test_original = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for future use\n",
    "model.save('artificial_subsidence_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Model for PIM Subsidence</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing <i>PIM_Subsidence</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset (assuming it's already cleaned and engineered as AI_Dataset.csv)\n",
    "data = pd.read_csv('PIM_Subsidence.csv')\n",
    "\n",
    "# Convert 'Stress Regime' categorical column to numerical using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['Stress_Regime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for NaNs and infinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME                     0\n",
      "SF                       0\n",
      "PUSHOVER LABEL           0\n",
      "PUSHOVER LAMBDA          0\n",
      "PUSHOVER U-CTRL          0\n",
      "                        ..\n",
      "PCA_1                    0\n",
      "PCA_2                    0\n",
      "PCA_3                    0\n",
      "PIM_Subsidence           0\n",
      "Stress_Regime_elastic    0\n",
      "Length: 75, dtype: int64\n",
      "TIME                     0\n",
      "SF                       0\n",
      "PUSHOVER LABEL           0\n",
      "PUSHOVER LAMBDA          0\n",
      "PUSHOVER U-CTRL          0\n",
      "                        ..\n",
      "PCA_1                    0\n",
      "PCA_2                    0\n",
      "PCA_3                    0\n",
      "PIM_Subsidence           0\n",
      "Stress_Regime_elastic    0\n",
      "Length: 75, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())\n",
    "print(data.applymap(np.isinf).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Normalizing <i>PIM_Subsidence</i> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "subsidence_scaler = StandardScaler()\n",
    "data['PIM_Subsidence'] = subsidence_scaler.fit_transform(data[['PIM_Subsidence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/ipykernel_launcher.py:20: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# Select both original and engineered features for the LSTM model\n",
    "features = data[['Displacement-Y','Eff.Stress-XY', 'Fluid velocity-Y', 'Solid-Velocity-Y', 'Saturation', 'Pore pressure',\n",
    "                 'Gradient_DisplacementY_X', 'Gradient_StressYY_X', 'Gradient_StressYY_Y', \n",
    "                 'Normalized_DisplacementY', 'Normalized_StressYY', 'Displacement-Magnitude', \n",
    "                 'Cumulative-Displacement-Y', 'StressYY_DisplacementY_Interaction', 'DisplacementY_Rate', \n",
    "                 'StressYY_Rate', 'Cumulative_StressYY', 'Log_StressYY', 'PCA_1', 'PCA_2', 'PCA_3'] + \n",
    "                 [col for col in data.columns if 'Stress_Regime' in col]]\n",
    "\n",
    "# Target variable\n",
    "target = data['PIM_Subsidence']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features (necessary for LSTM to learn effectively)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input data to 3D for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 1, X_valid.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Reshape targets to match LSTM input requirements\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_valid = y_valid.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model has been uploaded from saved file - do not execute, as it will create a fresh and untrained version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 256)            285696    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 256)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 482,945\n",
      "Trainable params: 482,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from time import time  # Import the time function\n",
    "\n",
    "# Define the LSTM model\n",
    "model = tf.keras.models.Sequential()\n",
    "# Adding LSTM layers with increased dropout\n",
    "model.add(tf.keras.layers.LSTM(units=256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(tf.keras.layers.Dropout(0.4))  # Increased dropout rate\n",
    "model.add(tf.keras.layers.LSTM(units=128, return_sequences=False))\n",
    "model.add(tf.keras.layers.Dropout(0.4))  # Increased dropout rate\n",
    "# Add Dense layer for output prediction\n",
    "model.add(tf.keras.layers.Dense(units=1))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Early Stopping with Patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Learning Rate Adjustment\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Model Checkpointing\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "# Data Augmentation\n",
    "def augment_data(X, y, noise_factor=0.05):\n",
    "    noise = noise_factor * np.random.randn(*X.shape)\n",
    "    X_augmented = X + noise\n",
    "    return X_augmented, y\n",
    "\n",
    "X_train_aug, y_train_aug = augment_data(X_train, y_train)\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate_and_train(model, X, y, epochs, batch_size, callbacks):\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), \n",
    "                            epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running TensorBoard in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "280/280 - 10s - loss: 0.0453 - val_loss: 0.0016 - lr: 1.0000e-05 - 10s/epoch - 37ms/step\n",
      "Epoch 2/500\n",
      "280/280 - 9s - loss: 0.0450 - val_loss: 0.0016 - lr: 1.0000e-05 - 9s/epoch - 31ms/step\n",
      "Epoch 3/500\n",
      "280/280 - 7s - loss: 0.0449 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 27ms/step\n",
      "Epoch 4/500\n",
      "280/280 - 7s - loss: 0.0439 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 5/500\n",
      "280/280 - 7s - loss: 0.0472 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 6/500\n",
      "280/280 - 8s - loss: 0.0443 - val_loss: 0.0016 - lr: 1.0000e-05 - 8s/epoch - 27ms/step\n",
      "Epoch 7/500\n",
      "280/280 - 7s - loss: 0.0439 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 8/500\n",
      "280/280 - 7s - loss: 0.0447 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 9/500\n",
      "280/280 - 8s - loss: 0.0455 - val_loss: 0.0016 - lr: 1.0000e-05 - 8s/epoch - 27ms/step\n",
      "Epoch 10/500\n",
      "280/280 - 9s - loss: 0.0468 - val_loss: 0.0016 - lr: 1.0000e-05 - 9s/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "280/280 - 8s - loss: 0.0468 - val_loss: 0.0016 - lr: 1.0000e-05 - 8s/epoch - 27ms/step\n",
      "Epoch 12/500\n",
      "280/280 - 7s - loss: 0.0445 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 13/500\n",
      "280/280 - 7s - loss: 0.0456 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 - 7s - loss: 0.0315 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 2/500\n",
      "280/280 - 7s - loss: 0.0265 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 3/500\n",
      "280/280 - 7s - loss: 0.0291 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 4/500\n",
      "280/280 - 7s - loss: 0.0291 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 5/500\n",
      "280/280 - 7s - loss: 0.0282 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 6/500\n",
      "280/280 - 8s - loss: 0.0295 - val_loss: 0.0774 - lr: 1.0000e-05 - 8s/epoch - 30ms/step\n",
      "Epoch 7/500\n",
      "280/280 - 7s - loss: 0.0306 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 8/500\n",
      "280/280 - 7s - loss: 0.0282 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 9/500\n",
      "280/280 - 7s - loss: 0.0310 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 10/500\n",
      "280/280 - 7s - loss: 0.0265 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 11/500\n",
      "280/280 - 7s - loss: 0.0294 - val_loss: 0.0774 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 - 7s - loss: 0.0487 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 2/500\n",
      "280/280 - 7s - loss: 0.0443 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 3/500\n",
      "280/280 - 7s - loss: 0.0469 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 4/500\n",
      "280/280 - 7s - loss: 0.0516 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 5/500\n",
      "280/280 - 7s - loss: 0.0464 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 6/500\n",
      "280/280 - 7s - loss: 0.0452 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 7/500\n",
      "280/280 - 7s - loss: 0.0454 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 8/500\n",
      "280/280 - 7s - loss: 0.0504 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 9/500\n",
      "280/280 - 7s - loss: 0.0471 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 10/500\n",
      "280/280 - 7s - loss: 0.0474 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 11/500\n",
      "280/280 - 7s - loss: 0.0471 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 12/500\n",
      "280/280 - 7s - loss: 0.0481 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 13/500\n",
      "280/280 - 7s - loss: 0.0503 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 14/500\n",
      "280/280 - 7s - loss: 0.0474 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 15/500\n",
      "280/280 - 7s - loss: 0.0500 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 16/500\n",
      "280/280 - 7s - loss: 0.0492 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 17/500\n",
      "280/280 - 7s - loss: 0.0469 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 18/500\n",
      "280/280 - 7s - loss: 0.0435 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 19/500\n",
      "280/280 - 7s - loss: 0.0468 - val_loss: 0.0016 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 - 7s - loss: 0.0453 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 2/500\n",
      "280/280 - 7s - loss: 0.0457 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 3/500\n",
      "280/280 - 7s - loss: 0.0447 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 4/500\n",
      "280/280 - 7s - loss: 0.0456 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 5/500\n",
      "280/280 - 7s - loss: 0.0508 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 6/500\n",
      "280/280 - 7s - loss: 0.0453 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 7/500\n",
      "280/280 - 7s - loss: 0.0467 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 8/500\n",
      "280/280 - 7s - loss: 0.0456 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 9/500\n",
      "280/280 - 7s - loss: 0.0455 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 10/500\n",
      "280/280 - 7s - loss: 0.0448 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 11/500\n",
      "280/280 - 7s - loss: 0.0439 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 12/500\n",
      "280/280 - 7s - loss: 0.0467 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 13/500\n",
      "280/280 - 7s - loss: 0.0463 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 14/500\n",
      "280/280 - 7s - loss: 0.0471 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 15/500\n",
      "280/280 - 7s - loss: 0.0439 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 16/500\n",
      "280/280 - 7s - loss: 0.0448 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 17/500\n",
      "280/280 - 7s - loss: 0.0475 - val_loss: 0.0020 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 - 7s - loss: 0.0280 - val_loss: 0.0780 - lr: 1.0000e-05 - 7s/epoch - 26ms/step\n",
      "Epoch 2/500\n",
      "280/280 - 7s - loss: 0.0266 - val_loss: 0.0780 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 3/500\n",
      "280/280 - 7s - loss: 0.0279 - val_loss: 0.0780 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 4/500\n",
      "280/280 - 7s - loss: 0.0301 - val_loss: 0.0780 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 5/500\n",
      "280/280 - 7s - loss: 0.0292 - val_loss: 0.0780 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 6/500\n",
      "280/280 - 7s - loss: 0.0268 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 24ms/step\n",
      "Epoch 7/500\n",
      "280/280 - 7s - loss: 0.0247 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 8/500\n",
      "280/280 - 7s - loss: 0.0321 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 9/500\n",
      "280/280 - 7s - loss: 0.0270 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 10/500\n",
      "280/280 - 7s - loss: 0.0269 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "Epoch 11/500\n",
      "280/280 - 7s - loss: 0.0291 - val_loss: 0.0781 - lr: 1.0000e-05 - 7s/epoch - 25ms/step\n",
      "350/350 [==============================] - 3s 5ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_768/674243583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m'Train Predictions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlstm_train_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m'Test Predictions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlstm_test_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m'Validation Predictions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlstm_valid_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m })\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     return arrays_to_mgr(\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Train the model with cross-validation\n",
    "history = cross_validate_and_train(model, X_train, y_train, epochs=500, batch_size=32, \n",
    "                                   callbacks=[early_stopping, lr_reduction, checkpoint, tensorboard])\n",
    "\n",
    "# Save the trained model for future use\n",
    "model.save('PIM_subsidence_lstm_model.h5')\n",
    "\n",
    "# Store predictions\n",
    "lstm_train_predictions = model.predict(X_train).flatten()\n",
    "lstm_test_predictions = model.predict(X_test).flatten()\n",
    "\n",
    "# Optionally, you can also store validation predictions\n",
    "lstm_valid_predictions = model.predict(X_valid).flatten()\n",
    "\n",
    "# Create a DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Train Predictions': lstm_train_predictions,\n",
    "    'Test Predictions': lstm_test_predictions,\n",
    "    'Validation Predictions': lstm_valid_predictions\n",
    "})\n",
    "\n",
    "# Export to CSV\n",
    "predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "train_predictions_df = pd.DataFrame({'Train Predictions': lstm_train_predictions})\n",
    "test_predictions_df = pd.DataFrame({'Test Predictions': lstm_test_predictions})\n",
    "validation_predictions_df = pd.DataFrame({'Validation Predictions': lstm_valid_predictions})\n",
    "    \n",
    "\n",
    "\n",
    "# Export to CSV\n",
    "train_predictions_df.to_csv('train_predictions.csv', index=False)\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "validation_predictions_df.to_csv('validation_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LSTM model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model for future use\n",
    "model.save('PIM_subsidence_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('PIM_subsidence_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With <i>TensorFlow/Keras Model Plotting</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With <i>PlotNeuralNet</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install LaTex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking LaTex\n",
    "$$\n",
    "f(x) = \\int_{-\\infty}^{\\infty} e^{-x^2} dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Clone PlotNeuralNet Repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/HarisIqbal88/PlotNeuralNet.git\n",
    "%cd PlotNeuralNet\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Navigate to the PlotNeuralNet directory\n",
    "%cd /resources/DL0120EN/labs/Week6/PlotNeuralNet\n",
    "\n",
    "# Add the PlotNeuralNet directory to Python path\n",
    "import sys\n",
    "sys.path.append('/resources/DL0120EN/labs/Week6/PlotNeuralNet')\n",
    "\n",
    "# Import the necessary modules\n",
    "from pycore.tikzeng import *\n",
    "from pycore.blocks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create Your LSTM Diagram:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the example.py script to match your LSTM model's architecture. You would need to tailor the blocks (e.g., ConvConvRelu, Pool, LSTM) to match the layers in your model. For an LSTM, you may use custom blocks that represent the LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your architecture\n",
    "arch = [\n",
    "    to_head('..'),\n",
    "    to_cor(),\n",
    "    to_begin(),\n",
    "\n",
    "    to_Conv(\"conv1\", 512, 64, offset=\"(0,0,0)\", to=\"(0,0,0)\", height=64, depth=64, width=2),\n",
    "    to_Pool(\"pool1\", offset=\"(0,0,0)\", to=\"(conv1-east)\"),\n",
    "    to_Conv(\"conv2\", 128, 64, offset=\"(1,0,0)\", to=\"(pool1-east)\", height=32, depth=32, width=2),\n",
    "    to_connection(\"pool1\", \"conv2\"),\n",
    "    to_Pool(\"pool2\", offset=\"(0,0,0)\", to=\"(conv2-east)\", height=28, depth=28, width=1),\n",
    "    to_SoftMax(\"soft1\", 10, \"(3,0,0)\", \"(pool2-east)\"),\n",
    "    to_end()\n",
    "]\n",
    "\n",
    "def main():\n",
    "    namefile = 'lstm_example'\n",
    "    to_generate(arch, namefile + '.tex')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and Render the Diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `IFrame` tool to convert the `.tex` file to a PDF and then display it in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import IFrame\n",
    "\n",
    "os.system(\"pdflatex lstm_example.tex\")\n",
    "IFrame(\"lstm_example.pdf\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `pymupdf` library to convert the PDF to a JPEG image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pymupdf\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import IPython.display as display\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(\"lstm_example.pdf\")\n",
    "\n",
    "# Select the first page\n",
    "page = pdf_document.load_page(0)\n",
    "\n",
    "# Convert the page to a pixmap (image)\n",
    "pix = page.get_pixmap()\n",
    "\n",
    "# Convert the pixmap to a PIL Image\n",
    "img = Image.open(BytesIO(pix.tobytes()))\n",
    "\n",
    "# Display the image in the Jupyter Notebook\n",
    "display.display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the .tex file using LaTeX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import IFrame\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Convert .tex to .pdf\n",
    "os.system(\"pdflatex lstm_example.tex\")\n",
    "\n",
    "# Display the PDF\n",
    "display(IFrame(\"lstm_example.pdf\", width=600, height=400))\n",
    "\n",
    "# Convert PDF to JPEG\n",
    "pages = convert_from_path('lstm_example.pdf', 300)\n",
    "for page in pages:\n",
    "    page.save('lstm_example.jpg', 'JPEG')\n",
    "\n",
    "# Display the JPEG\n",
    "from IPython.display import Image\n",
    "Image(filename='lstm_example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a PDF that visually represents your LSTM model. You can use this PDF in your reports or presentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the generated PDF in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"lstm_example.pdf\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
